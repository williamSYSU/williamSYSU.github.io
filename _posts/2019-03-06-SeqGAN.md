---
layout:         post
title:          "Note - SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"
subtitle:       "用GAN生成文本的开山之作"
date:           2019-03-06
author:         "William"
header-img:     "img/post-bg-sentiment-analysis.jpg"
catalog:        true
tags:
    - NLP
    - Paper Notes

typora-root-url: ..
---

## Before Reading

1. 在强化学习的策略梯度方法中，状态的转移概率是否一定要确定？还是因为本文用在了序列生成模型中，由模型的特点决定的？

   > 在策略梯度方法中，状态转移的概率可以是确定的，也可以是不确定的。在本文的模型中，状态定义为已生成的句子序列，动作定义为选下一个词，当执行动作确定后，下一个转移的状态也随之确定了。

2. 什么是策略梯度（Policy Gradient）？

   > 策略梯度来自于强化学习，其本质的思想是根据每个策略计算其奖励值，奖励值越大，其对应的策略被选择的可能性也越大。

3. 什么是蒙特卡罗搜索（Monte Carlo search）？在论文中的作用是什么？

   > 蒙特卡罗搜索方法的相关资料：
   >
   > - [AlphaGo背后的力量：蒙特卡洛树搜索入门指南——机器之心](https://www.jiqizhixin.com/articles/monte-carlo-tree-search-beginners-guide)
   > - [如何学习蒙特卡罗树搜索（MCTS）——tobe的知乎专栏](https://zhuanlan.zhihu.com/p/30458774)
   >
   > 蒙特卡罗搜索在论文中的作用：
   >
   > 用来评估中间状态的奖励值。即在生成完整的句子前，计算当前状态执行什么动作时获得的奖励最大。

4. 为什么SeqGAN中先预训练生成器G再预训练判别器D（在原始的GAN中是先D后G）？

   > 用MLE训练G是为了让G先模仿真实数据，这是为了学习，后面再用策略梯度更新G是为了让G生成与真实数据不一样的文本。如果一开始不训练G，直接用策略梯度更新，可能没办法拟合到真实数据中。MLE的目的是给G一定的预备知识。

5. 什么是intermedia reward，在[Sutton et al. 1999]中是否有提到？

   > 因为有一些奖励只有到了终止状态才能给出，而在终止状态之前计算得到的奖励值则成为中间奖励。在本文中，判别器只能在句子生成完成后才能给出奖励值，所以在本文中认为序列生成的问题是没有中间奖励的。

6. 论文的自动评价指标是什么？

   > 生成器的生成结果与随机初始化的LSTM（评估器）之间的差距

## Paper Core

本论文针对序列生成的问题，解决了原始的GAN在处理离散数据上的梯度很难回传的问题，提出了基于策略梯度的SeqGAN，利用蒙特卡罗搜索（Monte Carlo Search）和策略梯度（Policy Gradient）训练生成器G，通过判别器D计算奖励值，构成了GAN的基本形式，使得梯度能够有效回传并训练出一个有效的生成器G。



## Model

### 1. Introduction

- 为什么原始的GAN不能直接用于生成句子序列？

因为原始GAN只能处理连续的实值数据，如在图像处理中是直接对图像的像素值进行改变，但是对于句子的表示和图像的表示不同，句子中每个词的词向量数据是离散的，对词向量的进行数值操作后得到的向量是没有实际意义的，因为它无法对应到一个具体的词。在WGAN（Wasserstein GAN）[ Arjovsky et al., 2017 ]中也提出：**在（近似）最优判别器下，最小化生成器的loss等价于最小化真实分布和生成分布之间的JS散度，而由于真实分布与生成分布几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数log2，最终导致生成器的梯度（近似）为0，梯度消失。**【来源：[令人拍案叫绝的Wasserstein GAN——郑华滨的知乎专栏](https://zhuanlan.zhihu.com/p/25071913)】

尽管SeqGAN提出之时，WGAN还没有发表，但是我们从现在来看，原始的GAN处理的连续数据都存在这样的问题，那么对于词向量这类离散数据体现出的问题就更明显。



- 论文提出了什么方法来解决了什么问题？

原始GAN在序列生成中的问题：
a）无法处理离散数据，梯度无法回传；

> 将生成器视为一个带随机参数的策略，用强化学习中的策略梯度方法和蒙特卡罗搜索来训练生成器。

b）只能判别完整的句子，而对于只生成了一部分的词序列没办法判断当前生成序列的好坏，以及无法判断以后可能会生成的句子是否符合要求。

> 用蒙特卡罗方法补全未生成的序列，计算奖励值。



### 2. SeqGAN中的策略梯度

策略梯度的本质思想是根据策略计算对应的奖励值，如果某个策略的奖励值越大，在更新模型时，那么下一次用该策略的概率就会更大。在序列生成模型中，这里的策略定义为生成器G本身，例如当前参数状态的G生成一个序列，如果该序列所获得的奖励越大，则G的参数更新就会更偏向于生成这样的序列。

- 论文中的$Q_{D_{\phi}}^{G_{\theta}}(s,a)$是动作-值函数，表示在状态s下执行动作a后，执行策略（生成器G）后所获得的奖励期望。生成器G的目标是使生成的句子让判别器D的判别为真。
- 这里奖励值的设定是判别器D判断一个句子为真的概率。而因为D只能判别一个完整的句子，而模型又需要考虑每个时间步的奖励，即要同时考虑之前生成的奖励以及未来可能会获得的奖励。所以对于只生成了部分序列的句子，后面剩余部分用蒙特卡罗方法进行补全，从而将补全的句子输入D中计算奖励值（概率）。
- 蒙特卡罗搜索补全句子的过程：
  1. 假设我们要生成一个长度为T的句子，现已生成t个词，对于剩余的T-t个词，用蒙特卡罗方法来补全。在蒙特卡罗方法中，roll-out函数的作用是根据当前状态选择一个要执行的动作，在这里roll-out函数是生成器G本身，即生成器根据前面生成的句子，选择下一个生成词。注意，生成器G本质还是一个Seq2Seq的LSTM模型。
  2. 在执行roll-out函数（生成器G）时，将LSTM在第t个词输出的hidden state经过一个全连接和Softmax，得到词典中每个词的概率，然后根据**概率**选择下一个生成词（注意：这里跟翻译模型不一样的地方是，这里不是根据最大概率来选对应的词，而是根据对应的概率来选）。
  3. 以此类推，逐个生成T-t个词后，用判别器D计算奖励值，之后策略梯度根据奖励值来更新生成器的参数。
  4. 蒙特卡罗搜索的本质思想是大数定律，即经过足够多的采样后，其平均值接近理论的期望值。对应在这里就是我们都希望生成下一个词的时候获得较大的奖励值，但是我们并不清楚在哪个状态下生成哪个词的奖励最大，所以通过蒙特卡罗搜索的方法挑选出奖励期望最大的。所以在补全句子时，会运行多次蒙特卡罗方法，即补全多次句子，然后计算这些句子的平均奖励，以使下次执行策略时（生成器G）能获得更高的奖励值。
- 判别器D的目标函数与传统GAN中D的目标函数一样，都是尽量将生成句子判为假，将真实句子判为真。



### 3. SeqGAN的算法流程

SeqGAN的具体算法流程如下图所示：

![algorithm](/img/in-post/seqgan/image-20190307214817523.png)

- 用**MLE**训练G是为了让G先模仿真实数据，学习如何生成真实的句子，后面再用**策略梯度**更新G是为了让G生成与真实数据不一样的文本。如果一开始不训练G，直接用策略梯度更新，可能没办法拟合到真实数据中。MLE的目的是给G一定的预备知识。
- 在算法中是先预训练G再预训练D，而不同于原始GAN的先训练D再训练G，论文中没有刻意提到为什么要这么做。我的想法是，预训练G后，G生成负样本给D训练，这样D就能更准确地判别正例和负例。论文中有提到，进行有监督学习后的判别器D能更好地训练生成器G。
- 在g-steps循环里，首先生成**一个句子**，句子的生成是按Seq2Seq的方法生成，按**最大概率**生成下一个词，然后计算每个位置的词的奖励期望，计算奖励期望时用N次蒙特卡罗搜索补全该位置剩余的序列，继而得到该位置的奖励期望。然后用策略梯度的损失函数来更新生成器G。
- 在d-steps循环里，用更新后的G生成与正例数量相等的负例，然后正例和负例输入D训练k轮，其目标函数与原始GAN的判别器目标函数一致。



### 4. 生成器G

在前面已经提到，生成器G用的是传统Seq2Seq模型中的LSTM模型，其生成序列的方法与传统翻译模型的生成方法一样，将上一个时间步LSTM输出的hidden state经过全连接和Softmax，得到词典中每个词的概率。

在整个训练模型中，生成器G有两个作用：

- 生成句子序列：下一个生成词为Softmax结果中**概率最大**的词
- 作为蒙特卡罗搜索中的roll-out函数：下一个生成词为按Softmax结果中的**概率随机选择**结果



### 5. 判别器D

论文中的判别器用了[ Kim, 2014 ]中的CNN分类器。



### 6.  自动评价指标

论文中设置了一个随机初始化的LSTM模型作为具有先验知识的评估器，如果生成器生成的结果与LSTM模型生成的结果越接近，其结果也越好。



## Something can learn

借鉴强化学习中的策略梯度和蒙特卡罗搜索来解决原始GAN不能处理离散数据的问题，其想法的出发点是我们应该如何定义一个句子序列生成的过程，论文中想到生成每个词的过程就是做出一系列决策的过程，其任务与强化学习中的策略梯度方法一拍即合，而对于判别器仅能判别完整句子的问题上，作者想到了用蒙特卡罗搜索的方法来补全句子，这样就能计算整个句子的奖励值。



## Weak of model

1. 训练时不稳定，训练结果对G的D的训练次数比例敏感；
2. 无法生成多样化的句子，在DP-GAN [ Xu et al. 2018 ]中针对此问题对模型进行了改进； 



## Reference

*[ Yu et al., 2017 ] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.*

*[ Arjovsky et al., 2017 ] Martin tala, and Leon Bottou. Wasserstein GAN. CoRR abs/1701.07875, 2017.*

*[ Kim, 2014 ] Kim, Y. 2014. Convolutional neural networks for sentence classiﬁcation. arXiv:1408.5882.*

*[ Xu et al. 2018 ] Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text. arXiv preprint arXiv:1802.01345, 2018.* 